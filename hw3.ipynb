{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LDfWLvhYPtO"
   },
   "source": [
    "# Homework 3: Supervised machine learning\n",
    "\n",
    "UIC CS 418, Spring 2025\n",
    "\n",
    "_According to the **Academic Integrity Policy** of this course, all work submitted for grading must be done individually, unless otherwise specified. While we encourage you to talk to your peers and learn from them, this interaction must be superficial with regards to all work submitted for grading. This means you cannot work in teams, you cannot work side-by-side, you cannot submit someone else’s work (partial or complete) as your own. In particular, note that you are guilty of academic dishonesty if you extend or receive any kind of unauthorized assistance. Absolutely no transfer of program code between students is permitted (paper or electronic), and you may not solicit code from family, friends, or online forums. Other examples of academic dishonesty include emailing your program to another student, copying-pasting code from the internet, working in a group on a homework assignment, and allowing a tutor, TA, or another individual to write an answer for you. Academic dishonesty is unacceptable, and penalties range from failure to expulsion from the university; cases are handled via the official student conduct process described at https://dos.uic.edu/conductforstudents.shtml._\n",
    "\n",
    "_Authorship effort on any submitted work must be accurately documented. Artificial Intelligence tools like ChatGPT represent a new paradigm in academic learning. Their use is permitted for this assignment but should be used with caution. These systems often make mistakes and are best used when incorporated with user knowledge and experience, so try to actually learn the material in order to use these tools effectively. The main ideas and authorship of submitted work should be yours and the help must be clearly documented. At the end of this assignment, you are asked to write a \"Use of AI tools statement.\" You may want to look at the format of this statement before you start working on the homework, in order to understand what docomentation you are expected to provide._\n",
    "\n",
    "This homework is an individual assignment for all graduate students. Undergraduate students are allowed to work in pairs and submit one homework assignment per pair. There will be no extra credit given to undergraduate students who choose to work alone. The pairs of students who choose to work together and submit one homework assignment together still need to abide by the Academic Integrity Policy and not share or receive help from others (except each other).\n",
    "\n",
    "\n",
    "## Due Date\n",
    "\n",
    "This assignment is due at 11:59pm Friday, March 21st, 2025. Submitting the assignment any time between March 22nd and March 30th would incur one late day due to Spring Break. Submitting it on March 31st would incur two late days, and on April 1st - three late days.\n",
    "\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "You need to complete all code and answer all questions denoted by **Q#** (each one is under a building image) in this notebook. When you are done, you should export **`hw3.ipynb`** with your answers as a PDF file, upload that file `hw3.pdf` to *Homework 3 - Written Part* on Gradescope, tagging each question. \n",
    "\n",
    "You need to copy all functions that are part of questions Q1-Q7 and Q9 to `hw3.py`. That includes `process()`, `process_all()`, `create_features()`, `create_labels()`, `class MajorityLabelClassifier()`, `learn_classifier()`, `evaluate_classifier()` and `classify_tweets()`. You need to upload a completed Jupyter notebook (`hw3.ipynb` file) and `hw3.py` to *Homework 3 - code* on Gradescope. To help you get started, we have provided a template file (`hw3_template.py`) containing imports, some hints, and function skeletons.\n",
    "\n",
    "\n",
    "For undergraduate students who work in a team of two, only one student needs to submit the homework and just tag the other student on Gradescope.\n",
    "\n",
    "#### Autograding\n",
    "\n",
    "Questions will be graded based on both manual grading and an Autograder which will run on your `hw3.py` file. \n",
    "This assignment is graded on the basis of correctness and 64/100 points are given by the autograder. The remaining 31 points will be manually graded (7 points for Q8, 8 points for correctly running everything from Q1 to Q9 in the Jupyter notebook and 16 points for Q10 to Q14). You will finally be given 5 points for explaining whether and how you used AI tools.\n",
    "\n",
    "Most of the questions are graded independently in the autograder. This means that if you have an error in a question, it will not be propagated to another question. However, the Q9 will check your overall pipeline from Q1 to Q8 and is rather expensive to run on Gradescope. Therefore, you should disable its auto-grading on Gradescope until you have implemented and passed Q1 to Q7. A function `test_pipeline()` is provided in the hw3_template.py file that returns `False` by default to disable auto-grading of Q9. Once you complete the implementation of Q1 to Q7, you can enable auto-grading of the whole pipeline by setting `test_pipeline()` to return `True`.”\n",
    "\n",
    "The test cases will take a bit longer to execute. Make use of the resources wisely by first testing your functions in your notebook or making local test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vogrCA5aYPtQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import sklearn \n",
    "import string\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re # helps you filter urls\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "from IPython.display import display, Latex, Markdown\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PmFK0GgYPtU"
   },
   "source": [
    "# Classifying tweets [100%]\n",
    "\n",
    "In this problem, you will be analyzing Twitter data extracted using [this](https://dev.twitter.com/overview/api) api. The data contains tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`\n",
    "\n",
    "For every tweet, there are two pieces of information:\n",
    "- `screen_name`: the Twitter handle of the user tweeting and\n",
    "- `text`: the content of the tweet.\n",
    "\n",
    "The tweets have been divided into two parts - train and test available to you in CSV files. For train, both the `screen_name` and `text` attributes were provided but for test, `screen_name` is hidden.\n",
    "\n",
    "The overarching goal of the problem is to \"predict\" the political inclination (Republican/Democratic) of the Twitter user from one of his/her tweets. The ground truth (i.e., true class labels) is determined from the `screen_name` of the tweet as follows\n",
    "- `realDonaldTrump, mike_pence, GOP` are Republicans\n",
    "- `HillaryClinton, timkaine, TheDemocrats` are Democrats\n",
    "\n",
    "Thus, this is a binary classification problem. \n",
    "\n",
    "The problem proceeds in four stages. The first three stages include text processing, feature construction and tweet classification using a SVM classifier. The fourth stage includes the usage and performance of Large Language Model for tweet classification.\n",
    "- **Text processing (20%)**: We will clean up the raw tweet text using the various functions offered by the [nltk](http://www.nltk.org/genindex.html) package.\n",
    "- **Feature construction (20%)**: In this part, we will construct bag-of-words feature vectors and training labels from the processed text of tweets and the `screen_name` columns respectively.\n",
    "- **Classification (40%)**: Using the features derived, we will use [sklearn](http://scikit-learn.org/stable/modules/classes.html) package to learn a model which classifies the tweets as desired.\n",
    "- **Large Language Model for Tweet Classification (20%)**: We will explore [OpenAI](https://platform.openai.com/docs/overview) and [Gemini](https://aistudio.google.com/app/prompts/new_chat) LLM and assess the performance of LLM for tweet classification\n",
    "\n",
    "You will use two new python packages in this problem: `nltk` and `sklearn`, both of which should be available with anaconda. However, NLTK comes with many corpora, toy grammars, trained models, etc, which have to be downloaded manually. This assignment requires NLTK's stopwords list, POS tagger, and WordNetLemmatizer. Install them using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQGjuhliYPtV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harket/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/harket/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/harket/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/harket/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/harket/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/harket/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/harket/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "# Verify that the following commands work for you, before moving on.\n",
    "\n",
    "lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTqEaBw9YPtZ"
   },
   "source": [
    "Let's begin!\n",
    "\n",
    "## A. Text Processing [20%]\n",
    "\n",
    "You first task to fill in the following function which processes and tokenizes raw text. The generated list of tokens should meet the following specifications:\n",
    "1. The tokens must all be in lower case.\n",
    "2. The tokens should appear in the same order as in the raw text.\n",
    "3. The tokens must be in their lemmatized form. If a word cannot be lemmatized (i.e, you get an exception), simply catch it and ignore it. These words will not appear in the token list.\n",
    "4. The tokens must not contain any punctuations. Punctuations should be handled as follows: (a) Apostrophe of the form `'s` must be ignored. e.g., `She's` becomes `she`. (b) Other apostrophes should be omitted. e.g, `don't` becomes `dont`. (c) Words must be broken at the hyphen and other punctuations. \n",
    "5. The tokens must not contain any part of a url.\n",
    "\n",
    "Part of your work is to figure out a logical order to carry out the above operations. You may find `string.punctuation` useful, to get hold of all punctuation symbols. Look for [regular expressions](https://docs.python.org/3/library/re.html) capturing urls in the text. Your tokens must be of type `str`. Use `nltk.word_tokenize()` for tokenization once you have handled punctuation in the manner specified above. \n",
    "\n",
    "You would want to take a look at the `lemmatize()` function [here](https://www.nltk.org/_modules/nltk/stem/wordnet.html).\n",
    "In order for `lemmatize()` to give you the root form for any word, you have to provide the context in which you want to lemmatize through the `pos` parameter: `lemmatizer.lemmatize(word, pos=SOMEVALUE)`. The context should be the part of speech (POS) for that word. The good news is you do not have to manually write out the lexical categories for each word because [nltk.pos_tag()](https://www.nltk.org/book/ch05.html) will do this for you. Now you just need to use the results from `pos_tag()` for the `pos` parameter.\n",
    "However, you can notice the POS tag returned from `pos_tag()` is in different format than the expected pos by `lemmatizer`.\n",
    "> pos\n",
    "(Syntactic category): n for noun files, v for verb files, a for adjective files, r for adverb files.\n",
    "\n",
    "You need to map these pos appropriately. `nltk.help.upenn_tagset()` provides description of each tag returned by `pos_tag()`.\n",
    "\n",
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q1 (12%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5IMtsQoYPta"
   },
   "outputs": [],
   "source": [
    "# Convert part of speech tag from nltk.pos_tag to word net compatible format\n",
    "# Simple mapping based on first letter of return tag to make grading consistent\n",
    "# Everything else will be considered noun 'n'\n",
    "posMapping = {\n",
    "# \"First_Letter by nltk.pos_tag\":\"POS_for_lemmatizer\"\n",
    "    \"N\":'n',\n",
    "    \"V\":'v',\n",
    "    \"J\":'a',\n",
    "    \"R\":'r'\n",
    "}\n",
    "# 11% credits\n",
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"'s\\b\", \"\", text)\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [t for t in tokens if t.strip() != \"\"]\n",
    "    \n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_tokens = []\n",
    "    \n",
    "    # Lemmatize\n",
    "    for word, tag in tagged_tokens:\n",
    "        wn_tag = posMapping.get(tag[0].upper(), \"n\")  # default to noun\n",
    "        try:\n",
    "            lemma = lemmatizer.lemmatize(word, wn_tag)\n",
    "            final_tokens.append(lemma)\n",
    "        except Exception:\n",
    "            # If lemmatization fails, skip this token\n",
    "            continue\n",
    "    \n",
    "    return final_tokens\n",
    "    \n",
    "    #[YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bspITcPiYPte"
   },
   "source": [
    "You can test the above function as follows. Try to make your test strings as exhaustive as possible. Some checks are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrEzaolBYPtf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im', 'do', 'well', 'how', 'about', 'you']\n",
      "['education', 'be', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'lose', 'your', 'temper', 'or', 'your', 'self', 'confidence']\n",
      "['be', 'have', 'do', 'language', 'city', 'mice']\n",
      "['it', 'hilarious', 'check', 'it', 'out']\n",
      "['see', 'it', 'sunday', 'morning', 'at', '8', '30a', 'on', 'rtv6', 'and', 'our', 'rtv6', 'app', 'http', '…']\n"
     ]
    }
   ],
   "source": [
    "# 1% credit\n",
    "print(process(\"I'm doing well! How about you?\"))\n",
    "# ['im', 'do', 'well', 'how', 'about', 'you']\n",
    "\n",
    "print(process(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\"))\n",
    "# ['education', 'be', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'lose', 'your', 'temper', 'or', 'your', 'self', 'confidence']\n",
    "\n",
    "print(process(\"been had done languages cities mice\"))\n",
    "# ['be', 'have', 'do', 'language', 'city', 'mice']\n",
    "\n",
    "print(process(\"It's hilarious. Check it out http://t.co/dummyurl\"))\n",
    "# ['it', 'hilarious', 'check', 'it', 'out']\n",
    "\n",
    "print(process(\"See it Sunday morning at 8:30a on RTV6 and our RTV6 app. http:…\"))\n",
    "# ['see', 'it', 'sunday', 'morning', 'at', '8', '30a', 'on', 'rtv6', 'and', 'our', 'rtv6', 'app', 'http', '…']\n",
    "# Here '…' is a special unicode character not in string.punctuation and it is still present in processed text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ylcws_-LYPti"
   },
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q2 (8%):\n",
    "\n",
    "You will now use the `process()` function we implemented to convert the pandas dataframe we just loaded from tweets_train.csv file. Your function should be able to handle any data frame which contains a column called `text`. The data frame you return should replace every string in `text` with the result of `process()` and retain all other columns as such. Do not change the order of rows/columns. Before writing `process_all()`, load the data into a DataFrame and look at its format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEONT9fXYPti"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOP</td>\n",
       "      <td>RT @GOPconvention: #Oregon votes today. That m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheDemocrats</td>\n",
       "      <td>RT @DWStweets: The choice for 2016 is clear: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>Trump's calling for trillion dollar tax cuts f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HillaryClinton</td>\n",
       "      <td>.@TimKaine's guiding principle: the belief tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>timkaine</td>\n",
       "      <td>Glad the Senate could pass a #THUD / MilCon / ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      screen_name                                               text\n",
       "0             GOP  RT @GOPconvention: #Oregon votes today. That m...\n",
       "1    TheDemocrats  RT @DWStweets: The choice for 2016 is clear: W...\n",
       "2  HillaryClinton  Trump's calling for trillion dollar tax cuts f...\n",
       "3  HillaryClinton  .@TimKaine's guiding principle: the belief tha...\n",
       "4        timkaine  Glad the Senate could pass a #THUD / MilCon / ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "display(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SM7hJBdOYPtl"
   },
   "outputs": [],
   "source": [
    "# 7% credits\n",
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" process all text in the dataframe using process() function.\n",
    "    Inputs\n",
    "        df: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs\n",
    "        pd.DataFrame: dataframe in which the values of text column have been changed from str to list(str),\n",
    "                        the output from process() function. Other columns are unaffected.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()   \n",
    "    df_copy['text'] = df_copy['text'].apply(lambda t: process(t, lemmatizer))\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXQP6CAiYPto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      screen_name                                               text\n",
      "0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
      "1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, be, cl...\n",
      "2  HillaryClinton  [trump, call, for, trillion, dollar, tax, cut,...\n",
      "3  HillaryClinton  [timkaine, guide, principle, the, belief, that...\n",
      "4        timkaine  [glad, the, senate, could, pass, a, thud, milc...\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "# 1% credit\n",
    "processed_tweets = process_all(tweets)\n",
    "print(processed_tweets.head())\n",
    "\n",
    "#       screen_name                                               text\n",
    "# 0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
    "# 1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, be, cl...\n",
    "# 2  HillaryClinton  [trump, call, for, trillion, dollar, tax, cut,...\n",
    "# 3  HillaryClinton  [timkaine, guide, principle, the, belief, that...\n",
    "# 4        timkaine  [glad, the, senate, could, pass, a, thud, milc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsBiunCIYPtr"
   },
   "source": [
    "## B. Feature Construction [20%]\n",
    "\n",
    "The next step is to derive feature vectors from the tokenized tweets. In this section, you will be constructing a bag-of-words TF-IDF feature vector. But before that, as you may have guessed, the number of possible words is prohibitively large and not all of them may be useful for our classification task. We need to determine which words to retain, and which to omit. A common heuristic is to construct a frequency distribution of words in the corpus and prune out the head and tail of the distribution. The intuition of the above operation is as follows. Very common words (i.e. stopwords) add almost no information regarding similarity of two pieces of text. Similarly with very rare words. NLTK has a list of in-built stop words which is a good substitute for head of the distribution. We will consider a word rare if it occurs only in a single document (row) in whole of `tweets_train.csv`. \n",
    "\n",
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q3 (12%):\n",
    "\n",
    "Construct a sparse matrix of features for each tweet with the help of `sklearn.feature_extraction.text.TfidfVectorizer` (documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)). You need to pass a parameter `min_df=2` to filter out the words occuring only in one document in the whole training set. Remember to ignore the stop words as well. You must leave other optional parameters (e.g., `vocab`, `norm`, etc) at their default values. But you may need to use parameters like `lowercase` and `tokenizer` to handle `processed_tweets` that is a `list` of tokens (not raw text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NQ9wVm3YPt0"
   },
   "outputs": [],
   "source": [
    "# 11% credits\n",
    "def create_features(processed_tweets, stop_words):\n",
    "    \"\"\" creates the feature matrix using the processed tweet text\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: processed tweets read from train/test csv file, containing the column 'text'\n",
    "        stop_words: list(str): stop_words by nltk stopwords (after processing)\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "            we need this to tranform test tweets in the same way as train tweets\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    #\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=lambda x: x,          \n",
    "        preprocessor=lambda x: x,       \n",
    "        lowercase=False,               \n",
    "        stop_words=stop_words,         \n",
    "        min_df=2                     \n",
    "    )\n",
    "    \n",
    "    features = vectorizer.fit_transform(processed_tweets[\"text\"])\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXYtqEyvYPt4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TfidfVectorizer(lowercase=False, min_df=2,\n",
       "                 preprocessor=<function create_features.<locals>.<lambda> at 0x1756425c0>,\n",
       "                 stop_words=['a', 'about', 'above', 'after', 'again', 'against',\n",
       "                             'ain', 'all', 'be', 'an', 'and', 'any', 'be',\n",
       "                             'aren', 'arent', 'a', 'at', 'be', 'because', 'be',\n",
       "                             'before', 'be', 'below', 'between', 'both', 'but',\n",
       "                             'by', 'can', 'couldn', 'couldnt', ...],\n",
       "                 tokenizer=<function create_features.<locals>.<lambda> at 0x175641f80>),\n",
       " <17298x8720 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 165089 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute this code \n",
    "# 1% credit\n",
    "# It is recommended to process stopwords according to our data cleaning rules\n",
    "processed_stopwords = list(np.concatenate([process(word) for word in stopwords]))\n",
    "(tfidf, X) = create_features(processed_tweets, processed_stopwords)\n",
    "\n",
    "tfidf, X\n",
    "# Output (should be similar):\n",
    "# (TfidfVectorizer(lowercase=False, min_df=2,\n",
    "#                  stop_words=['a', 'about', 'above', 'after', 'again', 'against',\n",
    "#                              'ain', 'all', 'be', 'an', 'and', 'any', 'be',\n",
    "#                              'aren', 'arent', 'a', 'at', 'be', 'because', 'be',\n",
    "#                              'before', 'be', 'below', 'between', 'both', 'but',\n",
    "#                              'by', 'can', 'couldn', 'couldnt', ...],\n",
    "#                  tokenizer=<function tokenize at 0x000001DF22186C00>),\n",
    "#  <17298x8098 sparse matrix of type '<class 'numpy.float64'>'\n",
    "#  \twith 169627 stored elements in Compressed Sparse Row format>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WQJih7SYPt6"
   },
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q4 (8%):\n",
    "\n",
    "Also for each tweet, assign a class label (0 or 1) using its `screen_name`. Use 0 for realDonaldTrump, mike_pence, GOP and 1 for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLtDPaInYPt7"
   },
   "outputs": [],
   "source": [
    "# 7% credits\n",
    "def create_labels(processed_tweets):\n",
    "    \"\"\" creates the class labels from screen_name\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: tweets read from train file, containing the column 'screen_name'\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary numpy array of class labels\n",
    "    \"\"\"\n",
    "    names = {'realDonaldTrump', 'mike_pence', 'GOP'}\n",
    "    \n",
    "    labels = processed_tweets['screen_name'].apply(lambda name: 0 if name in names else 1)\n",
    "    \n",
    "    return labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z49a4djKYPt-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute this code\n",
    "# 1% credit\n",
    "y = create_labels(processed_tweets)\n",
    "y\n",
    "# 0        0\n",
    "# 1        1\n",
    "# 2        1\n",
    "# 3        1\n",
    "# 4        1\n",
    "#         ..\n",
    "# 17293    0\n",
    "# 17294    0\n",
    "# 17295    0\n",
    "# 17296    1\n",
    "# 17297    0\n",
    "# Name: screen_name, Length: 17298, dtype: int32\n",
    "# OR\n",
    "# array([0, 1, 1, ..., 0, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5STbtJYSYPuA"
   },
   "source": [
    "## C. Classification [39%]\n",
    "\n",
    "And finally, we are ready to put things together and learn a model for the classification of tweets. The classifier you will be using is [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) (Support Vector Machine). \n",
    "\n",
    "At the heart of SVMs is the concept of kernel functions, which determines how the similarity/distance between two data points in computed. `sklearn`'s SVM provides four kernel functions: `linear`, `poly`, `rbf`, `sigmoid` (details [here](http://scikit-learn.org/stable/modules/svm.html#svm-kernels)) but you can also implement your own distance function and pass it as an argument to the classifier.\n",
    "\n",
    "Through the various functions you implement in this part, you will be able to learn a classifier, score a classifier based on how well it performs, use it for prediction tasks and compare it to a baseline.\n",
    "\n",
    "Specifically, you will carry out the following tasks (Q5-9) in order:\n",
    "\n",
    "1. Implement and evaluate a simple baseline classifier MajorityLabelClassifier.\n",
    "2. Implement the `learn_classifier()` function assuming `kernel` is always one of {`linear`, `poly`, `rbf`, `sigmoid`}. \n",
    "3. Implement the `evaluate_classifier()` function which scores a classifier based on accuracy of a given dataset.\n",
    "4. Implement `best_model_selection()` to perform cross-validation by calling `learn_classifier()` and `evaluate_classifier()` for different folds and determine which of the four kernels performs the best.\n",
    "5. Go back to `learn_classifier()` and fill in the best kernel. \n",
    "\n",
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q5 (8%):\n",
    "\n",
    "To determine whether your classifier is performing well, you need to compare it to a baseline classifier. A baseline is generally a simple or trivial classifier and your classifier should beat the baseline in terms of a performance measure such as accuracy. Implement a classifier called `MajorityLabelClassifier` that always predicts the class equal to **mode** of the labels (i.e., the most frequent label) in training data. Part of the code is done for you. Implement the `fit` and `predict` methods. Initialize your classifier appropriately and evaluate it using the X and y variables created in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xUBpZ6_NYPuB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5001734304543878\n"
     ]
    }
   ],
   "source": [
    "# Skeleton of MajorityLabelClassifier is consistent with other sklearn classifiers\n",
    "# 7% credits\n",
    "class MajorityLabelClassifier():\n",
    "    \"\"\"\n",
    "    A classifier that predicts the mode of training labels\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize your parameter here\n",
    "        \"\"\"\n",
    "        self.mode_label = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Implement fit by taking training data X and their labels y and finding the mode of y\n",
    "        i.e. store your learned parameter\n",
    "        \"\"\"\n",
    "        num = np.bincount(y)\n",
    "        self.mode_label = np.argmax(num)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Implement to give the mode of training labels as a prediction for each data instance in X\n",
    "        return labels\n",
    "        \"\"\"\n",
    "        return np.full(shape=(X.shape[0],), fill_value=self.mode_label)\n",
    "\n",
    "# 1% credits\n",
    "# Report the accuracy of your classifier by comparing the predicted label of each example to its true label\n",
    "baselineClf = MajorityLabelClassifier()\n",
    "# Use fit and predict methods to get predictions and compare it with the true labels y\n",
    "baselineClf = MajorityLabelClassifier()\n",
    "baselineClf.fit(X, y)\n",
    "y_pred = baselineClf.predict(X)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(accuracy)\n",
    "# print(training accuracy) should give 0.5001734304543878"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZldwD1JYPuD"
   },
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q6 (8%):\n",
    "\n",
    "Implement the `learn_classifier()` function assuming `kernel` is always one of {`linear`, `poly`, `rbf`, `sigmoid`}. Stick to default values for any other optional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGpfOK2EYPuD"
   },
   "outputs": [],
   "source": [
    "# 7% credits\n",
    "def learn_classifier(X_train, y_train, kernel):\n",
    "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_labels()\n",
    "        kernel: str: kernel function to be used with classifier. [linear|poly|rbf|sigmoid]\n",
    "    Outputs:\n",
    "        sklearn.svm.SVC: classifier learnt from data\n",
    "    \"\"\"\n",
    "    classifier = SVC(kernel=kernel)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOwIKVuvYPuF"
   },
   "outputs": [],
   "source": [
    "# execute code\n",
    "# 1% credit\n",
    "classifier = learn_classifier(X, y, 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDsVwZuNYPuK"
   },
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q7 (8%):\n",
    "\n",
    "Now that we know how to learn a classifier, the next step is to evaluate it, ie., characterize how good its classification performance is. This step is necessary to select the best model among a given set of models, or even tune hyperparameters for a given model.\n",
    "\n",
    "There are two questions that should now come to your mind:\n",
    "1. **What data to use?** \n",
    "    - **Validation Data**: The data used to evaluate a classifier is called **validation data** (or hold-out data), and it is usually different from the data used for training. The model or hyperparameter with the best performance in the held out data is chosen. This approach is relatively fast and simple but vulnerable to biases found in validation set.\n",
    "    - **Cross-validation**: This approach divides the dataset in $k$ groups (so, called k-fold cross-validation). One of group is used as test set for evaluation and other groups as training set. The model or hyperparameter with the best average performance across all k folds is chosen. For this question you will perform 4-fold cross validation to determine the best kernel. We will keep all other hyperparameters default for now. This approach provides robustness toward biasness in validation set. However, it takes more time.\n",
    "    \n",
    "2. **And what metric?** There are several evaluation measures available in the literature (e.g., accuracy, precision, recall, F-1,etc) and different fields have different preferences for specific metrics due to different goals. We will go with accuracy. According to wiki, **accuracy** of a classifier measures the fraction of all data points that are correctly classified by it; it is the ratio of the number of correct classifications to the total number of (correct and incorrect) classifications. [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) provides a number of performance metrics..\n",
    "\n",
    "Now, implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvLsjxqKYPuL"
   },
   "outputs": [],
   "source": [
    "# 7% credits\n",
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "        X_validation: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_validation: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    predictions = classifier.predict(X_validation)\n",
    "    accuracy = accuracy_score(y_validation, predictions)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmdMkFIzYPuN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9562955254942768\n"
     ]
    }
   ],
   "source": [
    "# test your code by evaluating the accuracy on the training data\n",
    "# 1% credit\n",
    "accuracy = evaluate_classifier(classifier, X, y)\n",
    "print(accuracy) \n",
    "# should give around 0.9545034107989363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYYIADjZYPuP"
   },
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q8 (7%):\n",
    "\n",
    "Now it is time to decide which kernel works best by using the cross-validation technique. Write code to split the training data into 4-folds (75% training and 25% validation) by shuffling randomly. For each kernel, record the average accuracy for all folds and determine the best classifier. Since our dataset is balanced (both classes are in almost equal propertion), `sklearn.model_selection.KFold` [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) can be used for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nq5AlMoUYPuQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=4, random_state=1, shuffle=True)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)\n",
    "kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Og6hzUPPYPuT"
   },
   "source": [
    "Then use the following code to determine which classifier is the best. Remember that since we're performing cross-validation on four different models this code could take a while to execute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJAjQyNKYPuT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, Average Accuracy: 0.9045557929127922\n",
      "Kernel: rbf, Average Accuracy: 0.9132273692203217\n",
      "Kernel: poly, Average Accuracy: 0.9183148631378568\n",
      "Kernel: sigmoid, Average Accuracy: 0.8984856400357195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'poly'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8% credits\n",
    "def best_model_selection(kf, X, y):\n",
    "    \"\"\"\n",
    "    Select the kernel giving best results using k-fold cross-validation.\n",
    "    Other parameters should be left default.\n",
    "    Input:\n",
    "    kf (sklearn.model_selection.KFold): kf object defined above\n",
    "    X (scipy.sparse.csr.csr_matrix): training data\n",
    "    y (array(int)): training labels\n",
    "    Return:\n",
    "    best_kernel (string)\n",
    "    \"\"\"\n",
    "    best_kernel = None\n",
    "    best_accuracy = -1.0  # Initialize best accuracy to a very low number.\n",
    "    \n",
    "    for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n",
    "        fold_accuracies = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[test_index]\n",
    "            y_train, y_val = y[train_index], y[test_index]\n",
    "            \n",
    "            clf = learn_classifier(X_train, y_train, kernel)\n",
    "            \n",
    "            acc = evaluate_classifier(clf, X_val, y_val)\n",
    "            fold_accuracies.append(acc)\n",
    "        \n",
    "        avg_accuracy = np.mean(fold_accuracies)\n",
    "        print(f\"Kernel: {kernel}, Average Accuracy: {avg_accuracy}\")\n",
    "        \n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_kernel = kernel\n",
    "            \n",
    "    return best_kernel\n",
    "        # Use the documentation of KFold cross-validation to split ..\n",
    "        # training data and test data from create_features() and create_labels()\n",
    "        # call learn_classifer() using training split of kth fold\n",
    "        # evaluate on the test split of kth fold\n",
    "        # record avg accuracies and determine best model (kernel)\n",
    "    #return best kernel as string\n",
    "\n",
    "#Test your code\n",
    "best_kernel = best_model_selection(kf, X, y)\n",
    "best_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yho_UYX7YPuX"
   },
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q9 (8%)\n",
    "\n",
    "We're almost done! It's time to write a wrapper function that will use our model to classify unlabeled tweets from tweets_test.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qth7DPekYPuY"
   },
   "outputs": [],
   "source": [
    "# 7% credits\n",
    "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n",
    "    \"\"\" predicts class labels for raw tweet text\n",
    "    Inputs:\n",
    "        tfidf: sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used on training data\n",
    "        classifier: sklearn.svm.SVC: classifier learned\n",
    "        unlabeled_tweets: pd.DataFrame: tweets read from tweets_test.csv\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary vector of class labels for unlabeled tweets\n",
    "    \"\"\"\n",
    "    processed_texts = unlabeled_tweets[\"text\"].apply(process)\n",
    "    X_test = tfidf.transform(processed_texts)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKNMwVVgYPua"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
      " 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 1\n",
      " 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0\n",
      " 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1\n",
      " 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1\n",
      " 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0\n",
      " 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1\n",
      " 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
      " 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
      " 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
      " 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1\n",
      " 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1\n",
      " 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0\n",
      " 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0\n",
      " 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1\n",
      " 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0\n",
      " 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0\n",
      " 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1\n",
      " 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1\n",
      " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1\n",
      " 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# Fill in best classifier in your function and re-trian your classifier using all training data\n",
    "# Get predictions for unlabelled test data and print it\n",
    "# 1% credits\n",
    "classifier = learn_classifier(X, y, best_kernel)\n",
    "unlabeled_tweets = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
    "y_pred = classify_tweets(tfidf, classifier, unlabeled_tweets)\n",
    "\n",
    "#[YOUR CODE HERE]\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeuZhVR2OhyV"
   },
   "source": [
    "Did your SVM classifier perform better than the baseline (while evaluating with training data)? Explain in 1-2 sentences how you reached this conclusion.\n",
    "\n",
    "Yes, it was faster then baseline because SVM has a better accuracy becuase baseline classifier focuses on more frequent classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Can Large Language Models Do My Homework? [16%]\n",
    "\n",
    "Large language models have quickly advanced from often being \"laughably bad\" to \"quite capable\" for many tasks. Now, we will explore OpenAI LLM and assess the performance of LLM with our previous tweet classification machine learning model.\n",
    "\n",
    "In order to use OpenAI API, you need to install [`openai`](https://pypi.org/project/openai/) python module into your `cs418env`. You can install it using the command: `pip install openai`. After installing it into your `cs418env`, please researt the kernel form this notebook or close and open the notebook to take effect of the installation. Now, we can import the `openai` python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "from openai import OpenAI\n",
    "np.random.seed(4200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup OpenAI/Gemini API\n",
    "\n",
    "Visit this [link](https://aistudio.google.com/app/apikey) to create/login into your Google account and generate your API key for Gemini.\n",
    "\n",
    "Generating the API key will automatically create a project for you. Once it's done, copy/paste it into a file named 'gemini-key.txt' in your homework 3 directory. (Note: we are taking these steps to keep your keys private and excluded from your submitted PDF/files.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gemini-key.txt', 'r') as file:\n",
    "    gemini_key = file.read().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will set up a client for the OpenAI API with the api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=gemini_key,\n",
    "                base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are defined function for calling OpenAI/Gemini API endpoints for chat completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gemini(client, input_string, prompt=\"You are a helpful assistant.\", model=\"gemini-2.0-flash\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": prompt\n",
    "            },\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": input_string\n",
    "            }\n",
    "        ],\n",
    "        n=1\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the code that generates a prompt for the Gemini endpoint by combining an initial prompt with a numbered list of tweets. The prompt and number of tweets included are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(initial_prompt, tweets, length_limit):\n",
    "    prompt = initial_prompt\n",
    "    index = 0\n",
    "    for tweet in tweets:\n",
    "        if len(prompt) + len(tweet) + 5 < length_limit:\n",
    "            index = index + 1\n",
    "            prompt = prompt + \" \\n{}. \".format(index) + tweet\n",
    "    return prompt, index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q10: Defining a Prediction Prompt (2%)\n",
    "\n",
    "Define an appropriate prompt that requests a single value of 'R' or 'D' for Republican or Democrat so that LLM produces well-structured prediction label outputs for tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Classify each tweet as Republican (R) if they seem to have republican sentiment or Democrat (D) if they have democratic sentiment. \\n    For each tweet below, output only one letter (R or D) in order.\\n    Do not include any extra text. \\n1. Trump's calling for trillion dollar tax cuts for Wall Street. It's time for them to pay their fair share. \\n2. Obama is losing credibility with Syrian opposition leaders https://t.co/bANRRu2ktN \\n3. Positive relationships between faith groups &amp; law enforcement build more resilient communities https://t.co/6pfrSKVE72 \\n4. Tune in now to watch @JoeBiden hit the trail for Hillary in Ohio: https://t.co/FjCws9BTYy \\n5. Happy Birthday to a great President of the United States, George H. W. Bush! https://t.co/YOIB2eFfHG\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'DDRDR\\n'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Refine the `predict_initial_prompt` variable so that the LLM produces well-structured prediction label outputs\n",
    "predict_initial_prompt = (\"\"\"Classify each tweet as Republican (R) if they seem to have republican sentiment or Democrat (D) if they have democratic sentiment. \n",
    "    For each tweet below, output only one letter (R or D) in order.\n",
    "    Do not include any extra text.\"\"\")\n",
    "\n",
    "some_tweets = (\"Trump's calling for trillion dollar tax cuts for Wall Street. It's time for them to pay their fair share.\",\n",
    "          \"Obama is losing credibility with Syrian opposition leaders https://t.co/bANRRu2ktN\",\n",
    "          \"Positive relationships between faith groups &amp; law enforcement build more resilient communities https://t.co/6pfrSKVE72\",\n",
    "          \"Tune in now to watch @JoeBiden hit the trail for Hillary in Ohio: https://t.co/FjCws9BTYy\",\n",
    "          \"Happy Birthday to a great President of the United States, George H. W. Bush! https://t.co/YOIB2eFfHG\")\n",
    "\n",
    "prompt_predict, count = generate_prompt(predict_initial_prompt, some_tweets, 2048)\n",
    "display(prompt_predict)\n",
    "\n",
    "Gemini(client, prompt_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Prediction Pipeline\n",
    "\n",
    "In this part, we will construct the prediction pipeline that:  \n",
    "* Splits a test set of tweets into a list of prompts.\n",
    "* Parses the responses into predictions.\n",
    "\n",
    "First, choose a particular LLM model that you would like to use. We have set `gemini-2.0-flash` model by default in our `Gemini` function. You can use this model for the rest of the portion of this assignment but you are also free to choose any other LLM model from `OpenAI` if you want. Remember that this could require you to modify the `Gemini` function with additional parameters, so pay attention to the documentation for your model. Note that different models have different costs, so choose one that meets your budget. You can see different chat completions models [here](https://platform.openai.com/docs/models). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the model name of your choice\n",
    "model_name = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q11. Creating LLM prompts from tweets (4%).\n",
    "\n",
    "Using your `predict_initial_prompt` and repeated calls to `generate_prompt`, complete the function below that produces a list of LLM prompts given a list of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Classify each tweet as Republican (R) if they seem to have republican sentiment or Democrat (D) if they have democratic sentiment. \\n    For each tweet below, output only one letter (R or D) in order.\\n    Do not include any extra text. \\n1. “I get too many letters – from parents, and teachers, and kids – to sit around and do nothing.” #StopGunViolence\\nhttps://t.co/rpf0RWDAZJ \\n2. Article: Honda adding 100 jobs, investing $52 million in Greensburg facility #AStateThatWorks https://t.co/REhZgzxZgk \\n3. RT @marc_lotter: Despite John Gregg being in denial, Indiana's economy is running on all cylinders. @mike_pence  https://t.co/4xEaSZSF4F \\n4. RT @GovPenceIN: Just got out of #StarWarsTheForceAwakens, one word review: WOW \\n5. Why would the people of Florida vote for Marco Rubio when he defrauded them by agreeing to represent them as their Senator and then quit! \\n6. No child should have to say goodbye to their parents every morning not knowing if mom or dad will be there when they get home. \\n7. RT @GovPenceIN: Listen live as I join @WOWOCharly on @WOWOFORTWAYNE to discuss our legislative agenda https://t.co/xTZEc5kNSB \\n8. Donald Trump:\\n✔ Never wrong\\n✔ Never sorry\\n✔ Never responsible\\nhttps://t.co/yL0Cr9guR2 https://t.co/OVwe1lVINr \\n9. What's that you were saying about people being more than corporations, Donald? #BetterThanThis https://t.co/MbNspZTMsz \\n10. RT @chefjoseandres: @timkaine @DogTagBakery proud that my @WCKitchen is a small partner in their amazing success!!! \\n11. We will stop heroin and other drugs from coming into New Hampshire from our open southern border. We will build a WALL and have security. \\n12. RT @GovPenceIN: Great to be in studio with @tonykatz at my old stomping grounds @93wibc http://t.co/qSBrfKBnSR \\n13. In 2015, Trump launched his own campaign for president by describing Mexican immigrants as rapists and criminals. https://t.co/uSSd7XigYp \\n14. America stands for openness, freedom, and peace. \\nIslamophobia is un-American. 🇺🇸\\n#CNNDebate \\n15. Welcome to the #GOPDebate! https://t.co/aAc3uRpKg0\"]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_prompts(initial_prompt, tweets, length_limit):\n",
    "    prompts = []\n",
    "    current_batch = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        test_batch = current_batch + [tweet]\n",
    "        prompt, token_count = generate_prompt(initial_prompt, test_batch, length_limit)\n",
    "        \n",
    "        if token_count <= length_limit:\n",
    "            current_batch.append(tweet)\n",
    "        else:\n",
    "            if current_batch:\n",
    "                prompt, token_count = generate_prompt(initial_prompt, current_batch, length_limit)\n",
    "                prompts.append(prompt)\n",
    "            current_batch = [tweet]\n",
    "    \n",
    "    if current_batch:\n",
    "        prompt, token_count = generate_prompt(initial_prompt, current_batch, length_limit)\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return prompts\n",
    "    # [prompt, count] = generate_prompt(initial_prompt, tweets, length_limit)\n",
    "    # prompts.append(prompt)\n",
    "        \n",
    "    \n",
    "sample_tweets = tweets.sample(50)\n",
    "prompts = generate_prompts(predict_initial_prompt, sample_tweets[\"text\"].to_list(), 2048)\n",
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q12. Converting LLM Responses to Predictions (4%)\n",
    "\n",
    "Modify the following function to convert the LLM's response into a list of predictions for each provided tweet. Define test response strings like those produced by your particular LLM model to parse.\n",
    "\n",
    "For example, if your LLM output were strictly strings like \"DRRDRD\" then each character would be directly converted into a prediction label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'R', 'R', 'D', 'R', 'D']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['D', 'R', 'R', 'D', 'R', 'D']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def response_to_predictions(response_string):\n",
    "    predictions = []\n",
    "    \n",
    "    predictions = re.findall(r'[RD]', response_string)\n",
    "    # Define predictions based on the LLM response\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Test your response_to_predictions function here\n",
    "response1 = \"D\\nR\\nR\\nD\\nR\\nD\\n\"\n",
    "values1 = response_to_predictions(response1)\n",
    "display(values1)\n",
    "\n",
    "response2 = \"DRRDRD\"\n",
    "values2 = response_to_predictions(response2)\n",
    "display(values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q13. Processing Prompts and Responses (2%)\n",
    "\n",
    "Now, we will define function for processing prompts and produce predictions using Gemini API. Then, we will produce predictions for the 50 sampled tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt: Classify each tweet as Republican (R) if they seem to have republican sentiment or Democrat (D) if they have democratic sentiment. \n",
      "    For each tweet below, output only one letter (R or D) in order.\n",
      "    Do not include any extra text. \n",
      "1. “I get too many letters – from parents, and teachers, and kids – to sit around and do nothing.” #StopGunViolence\n",
      "https://t.co/rpf0RWDAZJ \n",
      "2. Article: Honda adding 100 jobs, investing $52 million in Greensburg facility #AStateThatWorks https://t.co/REhZgzxZgk \n",
      "3. RT @marc_lotter: Despite John Gregg being in denial, Indiana's economy is running on all cylinders. @mike_pence  https://t.co/4xEaSZSF4F \n",
      "4. RT @GovPenceIN: Just got out of #StarWarsTheForceAwakens, one word review: WOW \n",
      "5. Why would the people of Florida vote for Marco Rubio when he defrauded them by agreeing to represent them as their Senator and then quit! \n",
      "6. No child should have to say goodbye to their parents every morning not knowing if mom or dad will be there when they get home. \n",
      "7. RT @GovPenceIN: Listen live as I join @WOWOCharly on @WOWOFORTWAYNE to discuss our legislative agenda https://t.co/xTZEc5kNSB \n",
      "8. Donald Trump:\n",
      "✔ Never wrong\n",
      "✔ Never sorry\n",
      "✔ Never responsible\n",
      "https://t.co/yL0Cr9guR2 https://t.co/OVwe1lVINr \n",
      "9. What's that you were saying about people being more than corporations, Donald? #BetterThanThis https://t.co/MbNspZTMsz \n",
      "10. RT @chefjoseandres: @timkaine @DogTagBakery proud that my @WCKitchen is a small partner in their amazing success!!! \n",
      "11. We will stop heroin and other drugs from coming into New Hampshire from our open southern border. We will build a WALL and have security. \n",
      "12. RT @GovPenceIN: Great to be in studio with @tonykatz at my old stomping grounds @93wibc http://t.co/qSBrfKBnSR \n",
      "13. In 2015, Trump launched his own campaign for president by describing Mexican immigrants as rapists and criminals. https://t.co/uSSd7XigYp \n",
      "14. America stands for openness, freedom, and peace. \n",
      "Islamophobia is un-American. 🇺🇸\n",
      "#CNNDebate \n",
      "15. Welcome to the #GOPDebate! https://t.co/aAc3uRpKg0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D', 'R', 'R', 'R', 'D', 'D', 'R', 'D', 'D', 'D', 'R', 'R', 'D', 'D', 'R']"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tweets_to_predictions(predict_initial_prompt, tweet_list, model_name='gemini-2.0-flash', verbose=False):\n",
    "    all_predictions = []\n",
    "    prompt_list = generate_prompts(predict_initial_prompt, tweet_list, 2048)\n",
    "    for prompt in prompt_list:\n",
    "        if verbose:\n",
    "            print(\"Processing prompt: {}\".format(prompt))\n",
    "        response = Gemini(client, prompt, prompt=predict_initial_prompt, model=model_name)\n",
    "        # Sleep set to 5 seconds to avoid hitting the free tier rate limit of 15 requests per minute\n",
    "        time.sleep(5)\n",
    "        predictions = response_to_predictions(response)\n",
    "        all_predictions = all_predictions + predictions\n",
    "    return all_predictions\n",
    "\n",
    "# execute this code with your defined LLM model (2% credit)\n",
    "all_predictions = tweets_to_predictions(predict_initial_prompt, sample_tweets[\"text\"].to_list(), model_name=model_name, verbose=True)\n",
    "\n",
    "all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br>\n",
    "\n",
    "## Q14. Evaluating Prediction Accuracy and Takeaways (4%)\n",
    "\n",
    "Now, we will compare our SVM classifier performance with LLM model predictions. At first, we will split out training tweet data into 95% training and 5% test data randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets_sampled, test_tweets_sampled = sklearn.model_selection.train_test_split(tweets, test_size=0.05, random_state=4200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train a SVM classifer using the sampled 95% train data with our `best_kernel` from Q8 and evaluate performance of SVM classifier on the sampled 5% test data. Before running the cell below, make sure that you have completed from Q1-Q9 successfully as the below cell uses the whole pipeline of tweet classification. However, you do not need to change anything in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9283236994219654"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing train data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "train_tweets_sampled_processed = process_all(train_tweets_sampled)\n",
    "# creating features from train data\n",
    "(tfidf_1, X_train_sampled) = create_features(train_tweets_sampled_processed, processed_stopwords)\n",
    "# creating output labels for train data\n",
    "y_train_sampled = create_labels(train_tweets_sampled)\n",
    "# creating classifier using the best kernel\n",
    "classifier_1 = learn_classifier(X_train_sampled, y_train_sampled, best_kernel)\n",
    "\n",
    "# getting predictions from SVM classifier\n",
    "y_pred_sampled = classify_tweets(tfidf_1, classifier_1, test_tweets_sampled[['text']])\n",
    "# getting labels for test data\n",
    "y_test_sampled = create_labels(test_tweets_sampled)\n",
    "\n",
    "# calculating accuracy for the SVM classifier\n",
    "correct = 0\n",
    "for label, response in zip(y_test_sampled, y_pred_sampled):\n",
    "    if label == response:\n",
    "        correct += 1\n",
    "accuracy_svm = correct / len(y_pred_sampled)\n",
    "accuracy_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now get predictions from the LLM model for the sampled 5% test data and calculate the accuracy for the tweet classification. Remember that the below `tweets_to_predictions` function will make many API calls and may take several minutes (10-20 minutes) to complete. Please do not run the cell below too many times as it may cause you [rate limit](https://ai.google.dev/gemini-api/docs/rate-limits) error. Run the cell below after ensuring that you have completed all the above tasks for part D in this notebook and executed them properly without any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = tweets_to_predictions(predict_initial_prompt, test_tweets_sampled[\"text\"].to_list(), model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate the accuracy for LLM for the sampled 5% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Convert screen_names to labels for test_tweets_sampled dataframe using create_labels() function from Q4\n",
    "# Evaluate accuracy using all_predictions for LLM tweet predictions and print it\n",
    "y_test_sampled = create_labels(test_tweets_sampled)\n",
    "\n",
    "correct = 0\n",
    "for gold_label, pred_label in zip(y_test_sampled, all_predictions):\n",
    "    if gold_label == pred_label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy_llm = correct / len(all_predictions)\n",
    "print(accuracy_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did LLM prediction perform better than your SVM classifier? Explain in 2-3 sentences how you reached this conclusion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"whitehouse.png\" width=\"100px\" align=\"left\" float=\"left\"/>\n",
    "<br><br><br><br><br>\n",
    "\n",
    "\n",
    "## Q15: Statement on the use of AI tools (5%)\n",
    "\n",
    "You're almost done! Next, document your use of AI tools.\n",
    "\n",
    "Did you use any AI tools, such as ChatGPT, in the completion of **problems Q1-Q9** in this homework?\n",
    "\n",
    "Yes\n",
    "\n",
    "If you did, document **THOROUGHLY** which tools and how you used them. For each part of the homework that you used them, specify what prompts you used, whether you were satisfied with the tool's answer and whether you modified the tool's answer. \\\n",
    "NOTE: Not providing a complete answer will likely **NOT** earn you full credits for this question.\n",
    "\n",
    "Used google gemini to determine what packages from imports may be needed or are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:cs418env]",
   "language": "python",
   "name": "conda-env-cs418env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
